{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxdN8oRduSl7"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "finKJGOkuSl7"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoaUJqSOuSl8"
      },
      "source": [
        "**Read our [Gemma 3 blog](https://unsloth.ai/blog/gemma3) for what's new in Unsloth and our [Reasoning blog](https://unsloth.ai/blog/r1-reasoning) on how to train reasoning models.**\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIRveLJMuSl8"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jldR3ixXuSl8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3ZdWukguSl8"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228,
          "referenced_widgets": [
            "a4954bacd6d34aa283245fd19c2644fd",
            "6e8a7e86b3dd44088752e5a214397e11",
            "c6587efa49644386b5ae3c04766d9bc2",
            "57291316060f4f08b0b132272b726a02",
            "efebd10c2d5a41308bbff5569bcb4c25",
            "30992645b93d4c54baf8cdad6c4c929a",
            "4f88fc189a0e4c3e8a77daeeb11ef5c3",
            "9fcad95c256646cc8e40116fb52e73c0",
            "f95441ef1e5c4900af12687a4b9770a3",
            "906c329a38804ce4bf550ae4d8600fa5",
            "81ff5b4df3844f3f8a148cd6b9c13d8a",
            "2352c2b232a642739042ef80ed58f340",
            "6395fbb46e514e5fb4c9799ed49d5e1d",
            "d5b3294c4f364678995e3506c6d32851",
            "6b90a4ded41040e8b5ecf1e7db3c4d0e",
            "f713ffc3af7c43478b3ac3ab78902a2b",
            "85b0e4fceb974083be38d2432758bbe4",
            "c8f216c08a154083b8c3e57c5ae46789",
            "32a99360bcc24ac084d5bb8717b546af",
            "b0a140846ba14742ac35bc556e8cc7d4",
            "732a8e99a2b54250a897adf77dad92de",
            "29470a8b11b941ef83428b3aeb1a8b56"
          ]
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "2d1186be-93d5-4664-ca25-420ac15567f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/youcef/Documents/unsloth/.conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/youcef/.cache/huggingface/hub/models--unsloth--phi-4-unsloth-bnb-4bit/.no_exist/26dd1bdcaaab6bf52793b1a09b259ceed592d092/adapter_config.json'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 3090. Num GPUs = 1. Max memory: 23.545 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/youcef/.cache/huggingface/hub/models--unsloth--phi-4-unsloth-bnb-4bit/.no_exist/26dd1bdcaaab6bf52793b1a09b259ceed592d092/adapter_config.json'\n",
            "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/youcef/.cache/huggingface/hub/models--unsloth--phi-4-unsloth-bnb-4bit/.no_exist/26dd1bdcaaab6bf52793b1a09b259ceed592d092/adapter_config.json'\n",
            "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/home/youcef/.cache/huggingface/hub/models--unsloth--phi-4-unsloth-bnb-4bit/.no_exist/26dd1bdcaaab6bf52793b1a09b259ceed592d092/model.safetensors'\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:01<00:00,  2.19it/s]\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel  # FastVisionModel for LLMs\n",
        "import torch\n",
        "max_seq_length = 16384 # Choose any! We auto support RoPE Scaling internally!\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",  # Llama-3.1 2x faster\n",
        "    \"unsloth/Mistral-Small-Instruct-2409\",  # Mistral 22b 2x faster!\n",
        "    \"unsloth/Phi-4\",  # Phi-4 2x faster!\n",
        "    \"unsloth/Phi-4-unsloth-bnb-4bit\",  # Phi-4 Unsloth Dynamic 4-bit Quant\n",
        "    \"unsloth/gemma-2-9b-bnb-4bit\",  # Gemma 2x faster!\n",
        "    \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"  # Qwen 2.5 2x faster!\n",
        "    \"unsloth/Llama-3.2-1B-bnb-4bit\",  # NEW! Llama 3.2 models\n",
        "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
        "]  # More models at https://docs.unsloth.ai/get-started/all-our-models\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Phi-4\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bZsfBuZDeCL",
        "outputId": "f1e404a6-7697-4c74-c3c8-42cd7eafcbbd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.3.19 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the `Phi-4` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we convert it to HuggingFace's normal multiturn format `(\"role\", \"content\")` instead of `(\"from\", \"value\")`/ Phi-4 renders multi turn conversations like below:\n",
        "\n",
        "```\n",
        "<|im_start|>user<|im_sep|>Hello!<|im_end|>\n",
        "<|im_start|>assistant<|im_sep|>Hi! How can I help?<|im_end|>\n",
        "<|im_start|>user<|im_sep|>What is 2+2?<|im_end|>\n",
        "```\n",
        "\n",
        "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, phi4, llama3` and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "ba616d7ac9e045df89781674d862cfb2",
            "764638d0a3e64b4292ad06bab89df2e4",
            "7bfeafbc2fc843019187612ac61fda18",
            "f93bf0726a5a43aa8a2880fd8d488cb0",
            "d62c498931cf4239af39ebb460ba871a",
            "2cb3fc2097d94211a5fb330391e53cd1",
            "bbf5655c3e014fa69a71958d48747943",
            "05b0472385684402aca910ed2f0aabd9",
            "177e19c2009a4c4cbe44b3bd2db483b0",
            "843025172bf4478a8e5e8586385d55a6",
            "81fee9418ac8471c97267aa1654422e6",
            "2f6989a923464d6485656c80a7c37176",
            "84dd0711c1f449f183402a05fef703d6",
            "b4d63f8417e847f3b8a6f41a017bdf89",
            "c64d6a3a71264a5c99f39ab67196d2af",
            "68527325ed8c4d49a58b5466a84367fd",
            "0b985f5ef06542d387f98e0b098e0924",
            "6a3f08207d704c1e9186df24b359de83",
            "02d641481a7649f08a3a4e8f370fcfea",
            "53b54e72b7e547e38eddf9e73161e320",
            "780b64fb9953476585de3a34c734808b",
            "4f71cb58efbd4f33b8a65f543fb4976c",
            "ae77f6e84e69422bb914c0be452d86fc",
            "f9b25420835f430983b4bc81bdda34bb",
            "b048e885cb0d45db957f94586152665c",
            "c9804dfac114404dbc23e4f56f2979db",
            "1142781a8c3d4a1a91fc39a6f091984b",
            "6102523f0979436f884c6e71159b66e9",
            "635f4a58a057482c968b2a9701e626a6",
            "f59b04e983ed4bf1b6067a89a5364b87",
            "78451176ff984817a90e2680ffda2871",
            "72d316ffc8104f469f08667b47cb60b5",
            "fd142660e4554cb58b4ffcc716e97cde"
          ]
        },
        "id": "LjY75GoYUCB8",
        "outputId": "a7c267d9-de0c-476f-9d58-a25667e3e788"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['conversations'],\n",
              "    num_rows: 318\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "import json\n",
        "\n",
        "# Keep your existing tokenizer configuration\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"phi-4\",\n",
        ")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"conversations\"]\n",
        "    texts = [\n",
        "        tokenizer.apply_chat_template(\n",
        "            convo, tokenize = False, add_generation_prompt = False\n",
        "        )\n",
        "        for convo in convos\n",
        "    ]\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# Load your custom dataset from combined.json\n",
        "with open(\"combined.json\", \"r\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "# Convert the raw data into the format expected by the formatting function\n",
        "formatted_data = {\n",
        "    \"conversations\": raw_data\n",
        "}\n",
        "\n",
        "# Create a Dataset object from your formatted data\n",
        "from datasets import Dataset\n",
        "dataset = Dataset.from_dict(formatted_data)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "We now use `standardize_sharegpt` to convert ShareGPT style datasets into HuggingFace's generic format. This changes the dataset from looking like:\n",
        "```\n",
        "{\"from\": \"system\", \"value\": \"You are an assistant\"}\n",
        "{\"from\": \"human\", \"value\": \"What is 2+2?\"}\n",
        "{\"from\": \"gpt\", \"value\": \"It's 4.\"}\n",
        "```\n",
        "to\n",
        "```\n",
        "{\"role\": \"system\", \"content\": \"You are an assistant\"}\n",
        "{\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
        "{\"role\": \"assistant\", \"content\": \"It's 4.\"}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "8594b57f93d74a98ab3022077cf2fec8",
            "66624d56032a4dc4bfcd5114a0e25356",
            "3ed2452a3ca440fd9f8daee03a7fd219",
            "7529766ba1c14ec6bf6df35dcfc63218",
            "0ef50772c7a94db69138dab0dd358f84",
            "81a33502ef3e406f82c972c37b2cdf55",
            "e423bfaabc124dbabf79a67a61bc2a35",
            "cd89881e45884f2a9cf09cc09ac6a199",
            "b8410932ee414d12a0a687012421ae0b",
            "f0adc437d17b4e118d7d89b35f732081",
            "6a09a09ade794ced856efe7d44726925",
            "73dcd49b09a3489ab562443560bc5774",
            "8abd482bccf1494595e509ee89dc80b9",
            "1b5c4c7dfee84efeb977db2f599a8eaf",
            "d5860fd902644163942457dcc885da75",
            "7b9ab760372345c88630c7f4b9ff7094",
            "3cae835ecd5442c8a06f06a0d3e53d6c",
            "d7fdf432609f4615beab4f71b3da92ec",
            "60d69afaf00c4b2283507ff27e76327c",
            "57f56e28c0bd40c69e123d8e257197f0",
            "8047107bdb6840a08062d9fa5bfd9d0c",
            "27b1a069a17146ae9cb58a3df3bc5809"
          ]
        },
        "id": "oPXzJZzHEgXe",
        "outputId": "81e469fc-8ce4-4f70-dc70-e1a43b6322ef"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Standardizing formats (num_proc=20): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 318/318 [00:00<00:00, 1118.43 examples/s]\n",
            "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 318/318 [00:00<00:00, 18490.94 examples/s]\n"
          ]
        }
      ],
      "source": [
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndDUB23CGAC5"
      },
      "source": [
        "We look at how the conversations are structured for item 5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGFzmplrEy9I",
        "outputId": "5b79a38b-04e2-400b-8796-fd20fba788e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'content': 'You are an expert code analyzer. Your task is to analyze the provided AST structure and create a meaningful abstraction hierarchy.\\n\\nTASK: Analyze the provided AST-parsed code and create an abstraction hierarchy with logical groupings.\\n\\nINSTRUCTIONS:\\n1. Examine the provided AST structure with its nodes, text content, and references\\n2. Create a multi-level abstraction hierarchy where groups represent related functionality\\n   - The depth (number of nested groups) must be:\\n   - 1 (nodes have no children) for very simple code.\\n   - 2 (nodes have children) for most code.\\n   - at most 3 (nodes have children, and grandchildren) for very complex code.\\n3. For each group:\\n   - Create a concise 2-6 word descriptive label\\n   - Specify the range of node IDs covered (from first to last in the sequence)\\n   - Categorize with a single-word type that broadly describes its purpose. ONLY use one of these allowed types: \"documentation\", \"utility\", \"initialization\", \"execution\", \"callback\", \"validation\", \"visualization\", \"configuration\", \"processing\", \"security\", \"display\", \"terminal\", \"notification\", \"termination\", \"package\", \"messaging\", \"error\", \"file\", \"search\", \"loading\", \"folder\", \"conditional\", \"hardware\", \"network\"\\n4. Ensure each leaf node contains at most ONE reference in its range\\n5. Group related operations together rather than treating each line as its own group\\n6. Your ranges must include everything. Don\\'t skip any nodes.\\n7. Your output doesn\\'t have to be in the same order as the input, instead you should order them by the logical flow of the code. This is especially important for recursive functions, and callbacks \\n\\nIMPORTANT FORMATTING INSTRUCTIONS:\\n- You MUST respond with ONLY a valid JSON object\\n- idRange is a 2-element array of strings, representing the start and end of the range. Use the node IDs from the input. \\n   - IMPORTANT: must be exactly 2 elements, if start and end are the same, use the same ID twice.\\n- Do not include any explanations, markdown formatting, or additional text\\n- The JSON MUST follow be exactly of type `AbstractionTreeOutput`:\\n\\ntype AbstractionGroup = {\\n\\tlabel: string;\\n\\tidRange: [string, string];\\n\\ttype: string;\\n\\treferenceID?: string | null;\\n\\tchildren?: readonly AbstractionGroup[];\\n};\\n\\ntype AbstractionTreeOutput = {\\n\\toutput: readonly AbstractionGroup[];\\n};\\n\\n\\n### EXAMPLE INPUT:\\n{\\n  \"filePath\": \"/simple-app/src/utils.js\",\\n  \"context\": \"function handleData() { ... }\",\\n  \"ast\": [\\n    {\\n      \"id\": \"0\",\\n      \"text\": \"const data = fetchDataFromAPI()\"\\n\\t  \"references\": [\\n\\t\\t{\\n\\t\\t\\t\"symbol\": \"fetchDataFromAPI\",\\n\\t\\t\\t\"id\": \"hfuh2bda\"\\n\\t\\t}\\n\\t  ]\\n    },\\n    {\\n      \"id\": \"1\",\\n      \"text\": \"if (!data) { ... }\",\\n      \"children\": [\\n        {\\n          \"id\": \"1.0\",\\n          \"text\": \"console.error(\\'Failed to fetch data\\')\"\\n        },\\n        {\\n          \"id\": \"1.1\",\\n          \"text\": \"return null\"\\n        }\\n      ]\\n    },\\n    {\\n      \"id\": \"2\",\\n      \"text\": \"const processedData = processData(data)\",\\n      \"references\": [\\n        {\\n          \"symbol\": \"processData\",\\n          \"id\": \"abc123\"\\n        }\\n      ]\\n    },\\n    {\\n      \"id\": \"3\",\\n      \"text\": \"return processedData\"\\n    }\\n  ],\\n  \"references\": {\\n    \"abc123\": {\\n      \"shortBody\": \"function processData(rawData) { return rawData.map(item => item.value * 2) }\",\\n      \"symbol\": \"processData\"\\n    },\\n    \"hfuh2bda\": {\\n      \"shortBody\": \"function fetchDataFromAPI() { return fetch(\\'https://api.example.com/data\\').then(res => res.json()) }\",\\n      \"symbol\": \"fetchDataFromAPI\"\\n    }\\n  }\\n}\\n\\n### EXAMPLE OUTPUT:\\n{\\n  \"output\": [\\n\\t{\\n\\t\\t\"label\": \"Fetch data and return null if failed\",\\n\\t\\t\"idRange\": [\"0\", \"1.1\"],\\n\\t\\t\"type\": \"network\",\\n\\t\\t\"referenceID\": \"hfuh2bda\"\\n\\t},\\n\\t{\\n\\t\\t\"label\": \"Multiply all values by 2, and return result\",\\n\\t\\t\"idRange\": [\"2\", \"3\"],\\n\\t\\t\"type\": \"processing\",\\n\\t\\t\"referenceID\": \"abc123\"\\n\\t},\\n  ]\\n}\\n',\n",
              "  'role': 'system'},\n",
              " {'content': '{\\n  \"filePath\": \"C:/Users/taylor/code/git/mypy/mypy/modulefinder.py\",\\n  \"context\": \"class ModuleNotFoundReason(Enum):\\\\n    # The module was not found: we found neither stubs nor a plausible code\\\\n    # implementation (with or without a py.typed file).\\\\n    <class_definition_body/>\",\\n  \"ast\": [\\n    {\\n      \"id\": \"0\",\\n      \"text\": \"NOT_FOUND = 0\",\\n      \"children\": []\\n    },\\n    {\\n      \"id\": \"1\",\\n      \"text\": \"FOUND_WITHOUT_TYPE_HINTS = 1\",\\n      \"children\": []\\n    },\\n    {\\n      \"id\": \"2\",\\n      \"text\": \"WRONG_WORKING_DIRECTORY = 2\",\\n      \"children\": []\\n    },\\n    {\\n      \"id\": \"3\",\\n      \"text\": \"APPROVED_STUBS_NOT_INSTALLED = 3\",\\n      \"children\": []\\n    }\\n  ],\\n  \"references\": {}\\n}',\n",
              "  'role': 'user'},\n",
              " {'content': '{\"output\":[{\"label\":\"Module Not Found Reasons\",\"idRange\":[\"0\",\"3\"],\"type\":\"documentation\"}]}',\n",
              "  'role': 'assistant'}]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[5][\"conversations\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfzTdMtvGE6w"
      },
      "source": [
        "And we see how the chat template transformed these conversations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "vhXv0xFMGNKE",
        "outputId": "de5c57fb-8bea-4725-d120-997f2e6511ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|im_start|>system<|im_sep|>You are an expert code analyzer. Your task is to analyze the provided AST structure and create a meaningful abstraction hierarchy.\\n\\nTASK: Analyze the provided AST-parsed code and create an abstraction hierarchy with logical groupings.\\n\\nINSTRUCTIONS:\\n1. Examine the provided AST structure with its nodes, text content, and references\\n2. Create a multi-level abstraction hierarchy where groups represent related functionality\\n   - The depth (number of nested groups) must be:\\n   - 1 (nodes have no children) for very simple code.\\n   - 2 (nodes have children) for most code.\\n   - at most 3 (nodes have children, and grandchildren) for very complex code.\\n3. For each group:\\n   - Create a concise 2-6 word descriptive label\\n   - Specify the range of node IDs covered (from first to last in the sequence)\\n   - Categorize with a single-word type that broadly describes its purpose. ONLY use one of these allowed types: \"documentation\", \"utility\", \"initialization\", \"execution\", \"callback\", \"validation\", \"visualization\", \"configuration\", \"processing\", \"security\", \"display\", \"terminal\", \"notification\", \"termination\", \"package\", \"messaging\", \"error\", \"file\", \"search\", \"loading\", \"folder\", \"conditional\", \"hardware\", \"network\"\\n4. Ensure each leaf node contains at most ONE reference in its range\\n5. Group related operations together rather than treating each line as its own group\\n6. Your ranges must include everything. Don\\'t skip any nodes.\\n7. Your output doesn\\'t have to be in the same order as the input, instead you should order them by the logical flow of the code. This is especially important for recursive functions, and callbacks \\n\\nIMPORTANT FORMATTING INSTRUCTIONS:\\n- You MUST respond with ONLY a valid JSON object\\n- idRange is a 2-element array of strings, representing the start and end of the range. Use the node IDs from the input. \\n   - IMPORTANT: must be exactly 2 elements, if start and end are the same, use the same ID twice.\\n- Do not include any explanations, markdown formatting, or additional text\\n- The JSON MUST follow be exactly of type `AbstractionTreeOutput`:\\n\\ntype AbstractionGroup = {\\n\\tlabel: string;\\n\\tidRange: [string, string];\\n\\ttype: string;\\n\\treferenceID?: string | null;\\n\\tchildren?: readonly AbstractionGroup[];\\n};\\n\\ntype AbstractionTreeOutput = {\\n\\toutput: readonly AbstractionGroup[];\\n};\\n\\n\\n### EXAMPLE INPUT:\\n{\\n  \"filePath\": \"/simple-app/src/utils.js\",\\n  \"context\": \"function handleData() { ... }\",\\n  \"ast\": [\\n    {\\n      \"id\": \"0\",\\n      \"text\": \"const data = fetchDataFromAPI()\"\\n\\t  \"references\": [\\n\\t\\t{\\n\\t\\t\\t\"symbol\": \"fetchDataFromAPI\",\\n\\t\\t\\t\"id\": \"hfuh2bda\"\\n\\t\\t}\\n\\t  ]\\n    },\\n    {\\n      \"id\": \"1\",\\n      \"text\": \"if (!data) { ... }\",\\n      \"children\": [\\n        {\\n          \"id\": \"1.0\",\\n          \"text\": \"console.error(\\'Failed to fetch data\\')\"\\n        },\\n        {\\n          \"id\": \"1.1\",\\n          \"text\": \"return null\"\\n        }\\n      ]\\n    },\\n    {\\n      \"id\": \"2\",\\n      \"text\": \"const processedData = processData(data)\",\\n      \"references\": [\\n        {\\n          \"symbol\": \"processData\",\\n          \"id\": \"abc123\"\\n        }\\n      ]\\n    },\\n    {\\n      \"id\": \"3\",\\n      \"text\": \"return processedData\"\\n    }\\n  ],\\n  \"references\": {\\n    \"abc123\": {\\n      \"shortBody\": \"function processData(rawData) { return rawData.map(item => item.value * 2) }\",\\n      \"symbol\": \"processData\"\\n    },\\n    \"hfuh2bda\": {\\n      \"shortBody\": \"function fetchDataFromAPI() { return fetch(\\'https://api.example.com/data\\').then(res => res.json()) }\",\\n      \"symbol\": \"fetchDataFromAPI\"\\n    }\\n  }\\n}\\n\\n### EXAMPLE OUTPUT:\\n{\\n  \"output\": [\\n\\t{\\n\\t\\t\"label\": \"Fetch data and return null if failed\",\\n\\t\\t\"idRange\": [\"0\", \"1.1\"],\\n\\t\\t\"type\": \"network\",\\n\\t\\t\"referenceID\": \"hfuh2bda\"\\n\\t},\\n\\t{\\n\\t\\t\"label\": \"Multiply all values by 2, and return result\",\\n\\t\\t\"idRange\": [\"2\", \"3\"],\\n\\t\\t\"type\": \"processing\",\\n\\t\\t\"referenceID\": \"abc123\"\\n\\t},\\n  ]\\n}\\n<|im_end|><|im_start|>user<|im_sep|>{\\n  \"filePath\": \"C:/Users/taylor/code/git/mypy/mypy/modulefinder.py\",\\n  \"context\": \"class ModuleNotFoundReason(Enum):\\\\n    # The module was not found: we found neither stubs nor a plausible code\\\\n    # implementation (with or without a py.typed file).\\\\n    <class_definition_body/>\",\\n  \"ast\": [\\n    {\\n      \"id\": \"0\",\\n      \"text\": \"NOT_FOUND = 0\",\\n      \"children\": []\\n    },\\n    {\\n      \"id\": \"1\",\\n      \"text\": \"FOUND_WITHOUT_TYPE_HINTS = 1\",\\n      \"children\": []\\n    },\\n    {\\n      \"id\": \"2\",\\n      \"text\": \"WRONG_WORKING_DIRECTORY = 2\",\\n      \"children\": []\\n    },\\n    {\\n      \"id\": \"3\",\\n      \"text\": \"APPROVED_STUBS_NOT_INSTALLED = 3\",\\n      \"children\": []\\n    }\\n  ],\\n  \"references\": {}\\n}<|im_end|><|im_start|>assistant<|im_sep|>{\"output\":[{\"label\":\"Module Not Found Reasons\",\"idRange\":[\"0\",\"3\"],\"type\":\"documentation\"}]}<|im_end|>'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[5][\"text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "834b807ac41c4039b1bb8c9701b348ef",
            "fe48179cd7a54074b214ed8e3bda3108",
            "33c7d2c5e3994e86ba6acd49a8c06e00",
            "d540c030cbbc42f0a29a5c5a0cb6c04e",
            "f729a6f7a3fb4b028754a8d3605673bd",
            "ad23d13c83ed4c728e3cc1ad9ea60415",
            "3036b49d79774ff6ae809cf56a0af70d",
            "53458da298294ca0ad57b20a3f7c2e37",
            "436f412c800749338a63f3b4af1ae1be",
            "da2af48e73ad499db66105c95690b378",
            "a1517d1f49ba40e0befaeccfb8c087fc"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "f78923ac-b9fb-4548-d33f-0249167f8de3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Tokenizing [\"text\"] (num_proc=2): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 318/318 [00:00<00:00, 500.38 examples/s]\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 3, # Set this for 1 full training run.\n",
        "        max_steps = 90,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_sGp5XlG6dq"
      },
      "source": [
        "We also use Unsloth's `train_on_completions` method to only train on the assistant outputs and ignore the loss on the user's inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "623f6d9c54a6482c853233eb80799fe4",
            "0262bf690a744e34b348a1a7efb74b1f",
            "d13a77154efc4c3a85ae4411fa6ddf1c",
            "8c2ca52c23b84a8486155de435810fbf",
            "9e6ec29b77f448b392b8ff1a91997a8a",
            "fa980b3860db4e508140ec6073462cc9",
            "7153553bfd334094bcb67f8f4cc5d991",
            "84a035ac792d422592f253af976a8b6b",
            "90cdca8d013f41688dcc03967b47c377",
            "6e98be70d573433ca155e571e6a46c1e",
            "cfcd0cd808b4460185579afb6de0ece1"
          ]
        },
        "id": "juQiExuBG5Bt",
        "outputId": "fcc25e15-d3c4-4ef2-c993-3eb5db712573"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map (num_proc=20): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 318/318 [00:00<00:00, 837.45 examples/s]\n"
          ]
        }
      ],
      "source": [
        "from unsloth.chat_templates import train_on_responses_only\n",
        "\n",
        "trainer = train_on_responses_only(\n",
        "    trainer,\n",
        "    instruction_part=\"<|im_start|>user<|im_sep|>\",\n",
        "    response_part=\"<|im_start|>assistant<|im_sep|>\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv1NBUozV78l"
      },
      "source": [
        "We verify masking is actually done:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "LtsMVtlkUhja",
        "outputId": "6fa2529b-d480-49b3-b62a-1b7a38e5d4ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<|im_start|>system<|im_sep|>You are an expert code analyzer. Your task is to analyze the provided AST structure and create a meaningful abstraction hierarchy.\\n\\nTASK: Analyze the provided AST-parsed code and create an abstraction hierarchy with logical groupings.\\n\\nINSTRUCTIONS:\\n1. Examine the provided AST structure with its nodes, text content, and references\\n2. Create a multi-level abstraction hierarchy where groups represent related functionality\\n   - The depth (number of nested groups) must be:\\n   - 1 (nodes have no children) for very simple code.\\n   - 2 (nodes have children) for most code.\\n   - at most 3 (nodes have children, and grandchildren) for very complex code.\\n3. For each group:\\n   - Create a concise 2-6 word descriptive label\\n   - Specify the range of node IDs covered (from first to last in the sequence)\\n   - Categorize with a single-word type that broadly describes its purpose. ONLY use one of these allowed types: \"documentation\", \"utility\", \"initialization\", \"execution\", \"callback\", \"validation\", \"visualization\", \"configuration\", \"processing\", \"security\", \"display\", \"terminal\", \"notification\", \"termination\", \"package\", \"messaging\", \"error\", \"file\", \"search\", \"loading\", \"folder\", \"conditional\", \"hardware\", \"network\"\\n4. Ensure each leaf node contains at most ONE reference in its range\\n5. Group related operations together rather than treating each line as its own group\\n6. Your ranges must include everything. Don\\'t skip any nodes.\\n7. Your output doesn\\'t have to be in the same order as the input, instead you should order them by the logical flow of the code. This is especially important for recursive functions, and callbacks \\n\\nIMPORTANT FORMATTING INSTRUCTIONS:\\n- You MUST respond with ONLY a valid JSON object\\n- idRange is a 2-element array of strings, representing the start and end of the range. Use the node IDs from the input. \\n   - IMPORTANT: must be exactly 2 elements, if start and end are the same, use the same ID twice.\\n- Do not include any explanations, markdown formatting, or additional text\\n- The JSON MUST follow be exactly of type `AbstractionTreeOutput`:\\n\\ntype AbstractionGroup = {\\n\\tlabel: string;\\n\\tidRange: [string, string];\\n\\ttype: string;\\n\\treferenceID?: string | null;\\n\\tchildren?: readonly AbstractionGroup[];\\n};\\n\\ntype AbstractionTreeOutput = {\\n\\toutput: readonly AbstractionGroup[];\\n};\\n\\n\\n### EXAMPLE INPUT:\\n{\\n  \"filePath\": \"/simple-app/src/utils.js\",\\n  \"context\": \"function handleData() { ... }\",\\n  \"ast\": [\\n    {\\n      \"id\": \"0\",\\n      \"text\": \"const data = fetchDataFromAPI()\"\\n\\t  \"references\": [\\n\\t\\t{\\n\\t\\t\\t\"symbol\": \"fetchDataFromAPI\",\\n\\t\\t\\t\"id\": \"hfuh2bda\"\\n\\t\\t}\\n\\t  ]\\n    },\\n    {\\n      \"id\": \"1\",\\n      \"text\": \"if (!data) { ... }\",\\n      \"children\": [\\n        {\\n          \"id\": \"1.0\",\\n          \"text\": \"console.error(\\'Failed to fetch data\\')\"\\n        },\\n        {\\n          \"id\": \"1.1\",\\n          \"text\": \"return null\"\\n        }\\n      ]\\n    },\\n    {\\n      \"id\": \"2\",\\n      \"text\": \"const processedData = processData(data)\",\\n      \"references\": [\\n        {\\n          \"symbol\": \"processData\",\\n          \"id\": \"abc123\"\\n        }\\n      ]\\n    },\\n    {\\n      \"id\": \"3\",\\n      \"text\": \"return processedData\"\\n    }\\n  ],\\n  \"references\": {\\n    \"abc123\": {\\n      \"shortBody\": \"function processData(rawData) { return rawData.map(item => item.value * 2) }\",\\n      \"symbol\": \"processData\"\\n    },\\n    \"hfuh2bda\": {\\n      \"shortBody\": \"function fetchDataFromAPI() { return fetch(\\'https://api.example.com/data\\').then(res => res.json()) }\",\\n      \"symbol\": \"fetchDataFromAPI\"\\n    }\\n  }\\n}\\n\\n### EXAMPLE OUTPUT:\\n{\\n  \"output\": [\\n\\t{\\n\\t\\t\"label\": \"Fetch data and return null if failed\",\\n\\t\\t\"idRange\": [\"0\", \"1.1\"],\\n\\t\\t\"type\": \"network\",\\n\\t\\t\"referenceID\": \"hfuh2bda\"\\n\\t},\\n\\t{\\n\\t\\t\"label\": \"Multiply all values by 2, and return result\",\\n\\t\\t\"idRange\": [\"2\", \"3\"],\\n\\t\\t\"type\": \"processing\",\\n\\t\\t\"referenceID\": \"abc123\"\\n\\t},\\n  ]\\n}<|im_end|><|im_start|>user<|im_sep|>{\\n  \"filePath\": \"C:/Users/taylor/code/git/mypy/mypy/modulefinder.py\",\\n  \"context\": \"class ModuleNotFoundReason(Enum):\\\\n    # The module was not found: we found neither stubs nor a plausible code\\\\n    # implementation (with or without a py.typed file).\\\\n    <class_definition_body/>\",\\n  \"ast\": [\\n    {\\n      \"id\": \"0\",\\n      \"text\": \"NOT_FOUND = 0\",\\n      \"children\": []\\n    },\\n    {\\n      \"id\": \"1\",\\n      \"text\": \"FOUND_WITHOUT_TYPE_HINTS = 1\",\\n      \"children\": []\\n    },\\n    {\\n      \"id\": \"2\",\\n      \"text\": \"WRONG_WORKING_DIRECTORY = 2\",\\n      \"children\": []\\n    },\\n    {\\n      \"id\": \"3\",\\n      \"text\": \"APPROVED_STUBS_NOT_INSTALLED = 3\",\\n      \"children\": []\\n    }\\n  ],\\n  \"references\": {}\\n}<|im_end|><|im_start|>assistant<|im_sep|>{\"output\":[{\"label\":\"Module Not Found Reasons\",\"idRange\":[\"0\",\"3\"],\"type\":\"documentation\"}]}<|im_end|>'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "_rD6fl8EUxnG",
        "outputId": "f8602f78-9674-4911-d37b-03fc203e00b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  {\"output\":[{\"label\":\"Module Not Found Reasons\",\"idRange\":[\"0\",\"3\"],\"type\":\"documentation\"}]}<|im_end|>'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
        "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3enWUM0jV-jV"
      },
      "source": [
        "We can see the System and Instruction prompts are successfully masked!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "94e771b7-a863-4759-f5c5-390574cf8338"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = NVIDIA GeForce RTX 3090. Max memory = 23.545 GB.\n",
            "9.967 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "62ec7355-f48e-4615-a94c-4127bb68801b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 318 | Num Epochs = 3 | Total steps = 90\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 65,536,000/4,000,000,000 (1.64% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [90/90 37:19, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.684100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.197200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.478200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.336400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.650700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.276100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.249000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.330700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.198300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.224400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.312200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.243700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.248800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.192200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.293200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.194400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.203100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.284700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.226200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.197100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.215400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.215000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.194000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.274800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.231400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.162000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.184800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.219800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.148600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.172000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.195200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.188600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.221000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.387700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.244600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.140300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.176600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.158100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.218600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.542600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.164300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.195500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.115400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.133000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.153900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.136100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.136100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.197400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.208800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.172700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.188400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.148200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.164400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.175900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.151300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.204500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.128400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.185600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.186100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.124300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.185900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.166100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.162900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.182100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.161000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.194400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.197500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.151000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.168500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.184500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.177000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.111400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.211200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.147500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.199300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.151800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.157800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.197400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.256300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.159200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.164500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.115300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.215900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.165000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.174200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.130000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.148000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.178800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.132900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.148400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "b2d8b834-7957-4bb2-abc8-200b44280ab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2256.7512 seconds used for training.\n",
            "37.61 minutes used for training.\n",
            "Peak reserved memory = 13.947 GB.\n",
            "Peak reserved memory for training = 3.98 GB.\n",
            "Peak reserved memory % of max memory = 59.236 %.\n",
            "Peak reserved memory for training % of max memory = 16.904 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! You can change the instruction and input - leave the output blank!\n",
        "\n",
        "**[NEW] Try 2x faster inference in a free Colab for Llama-3.1 8b Instruct [here](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Unsloth_Studio.ipynb)**\n",
        "\n",
        "We use `min_p = 0.1` and `temperature = 1.5`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "aaff4281-4838-4bdf-f8c3-15136cc63398"
      },
      "outputs": [],
      "source": [
        "# from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "# tokenizer = get_chat_template(\n",
        "#     tokenizer,\n",
        "#     chat_template = \"phi-4\",\n",
        "# )\n",
        "# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# messages = [\n",
        "#     {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
        "# ]\n",
        "# inputs = tokenizer.apply_chat_template(\n",
        "#     messages,\n",
        "#     tokenize = True,\n",
        "#     add_generation_prompt = True, # Must add for generation\n",
        "#     return_tensors = \"pt\",\n",
        "# ).to(\"cuda\")\n",
        "\n",
        "# outputs = model.generate(\n",
        "#     input_ids = inputs, max_new_tokens = 64, use_cache = True, temperature = 1.5, min_p = 0.1\n",
        "# )\n",
        "# tokenizer.batch_decode(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2pEuRb1r2Vg",
        "outputId": "812b3637-3aa2-446e-c01b-f50e5787927b"
      },
      "outputs": [],
      "source": [
        "# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# messages = [\n",
        "#     {\"role\": \"user\", \"content\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
        "# ]\n",
        "# inputs = tokenizer.apply_chat_template(\n",
        "#     messages,\n",
        "#     tokenize = True,\n",
        "#     add_generation_prompt = True, # Must add for generation\n",
        "#     return_tensors = \"pt\",\n",
        "# ).to(\"cuda\")\n",
        "\n",
        "# from transformers import TextStreamer\n",
        "# text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "# _ = model.generate(\n",
        "#     input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "#     use_cache = True, temperature = 1.5, min_p = 0.1\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "82b0db99-a5df-4ab8-a73a-ecfe1174a022"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/vocab.json',\n",
              " 'lora_model/merges.txt',\n",
              " 'lora_model/added_tokens.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKX_XKs_BNZR",
        "outputId": "bba3c600-0fb1-4e09-da10-2415d4463232"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "# messages = [\n",
        "#     {\"role\": \"user\", \"content\": \"Describe a tall tower in the capital of France.\"},\n",
        "# ]\n",
        "# inputs = tokenizer.apply_chat_template(\n",
        "#     messages,\n",
        "#     tokenize = True,\n",
        "#     add_generation_prompt = True, # Must add for generation\n",
        "#     return_tensors = \"pt\",\n",
        "# ).to(\"cuda\")\n",
        "\n",
        "# from transformers import TextStreamer\n",
        "# text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "# _ = model.generate(\n",
        "#     input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "#     use_cache = True, temperature = 1.5, min_p = 0.1\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit=load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Merge to 16bit\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m: model\u001b[38;5;241m.\u001b[39msave_pretrained_merged(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, tokenizer, save_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmerged_16bit\u001b[39m\u001b[38;5;124m\"\u001b[39m,)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list in our [Docs](https://docs.unsloth.ai/basics/saving-and-using-models/saving-to-gguf)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 6.3 out of 31.11 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|‚ñà‚ñà‚ñà‚ñå      | 14/40 [00:00<00:00, 32.18it/s]\n",
            "We will save to Disk and not RAM now.\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 40/40 [00:14<00:00,  2.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Converting llama model. Can use fast conversion = False.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q4_k_m', 'q5_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: [1] Converting model at model into bf16 GGUF format.\n",
            "The output location will be /home/youcef/Documents/unsloth/model/unsloth.BF16.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: model\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00001-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {5120, 100352}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00002-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00003-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00004-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00005-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.32.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.32.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.32.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.32.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.32.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.32.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.32.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.33.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.33.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.33.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.33.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.33.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.33.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.33.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.33.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.33.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.34.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.34.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.34.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.34.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.34.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'model-00006-of-00006.safetensors'\n",
            "INFO:hf-to-gguf:output.weight,               torch.bfloat16 --> BF16, shape = {5120, 100352}\n",
            "INFO:hf-to-gguf:blk.34.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.34.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.34.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.35.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.35.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.35.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.35.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.35.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.35.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.36.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.36.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.36.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.36.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.36.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.36.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.36.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.37.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.37.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.37.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.37.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.37.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.37.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.37.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.38.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.38.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.38.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.38.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.38.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.38.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.38.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.39.attn_norm.weight,     torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.39.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {17920, 5120}\n",
            "INFO:hf-to-gguf:blk.39.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.39.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {5120, 17920}\n",
            "INFO:hf-to-gguf:blk.39.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_k.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:blk.39.attn_output.weight,   torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_q.weight,        torch.bfloat16 --> BF16, shape = {5120, 5120}\n",
            "INFO:hf-to-gguf:blk.39.attn_v.weight,        torch.bfloat16 --> BF16, shape = {5120, 1280}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {5120}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 16384\n",
            "INFO:hf-to-gguf:gguf: embedding length = 5120\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 17920\n",
            "INFO:hf-to-gguf:gguf: head count = 40\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 10\n",
            "INFO:hf-to-gguf:gguf: rope theta = 250000\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 32\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:hf-to-gguf:'√Ø¬ø¬Ω' is encoded and decoded back to 'ÔøΩ' using AutoTokenizer\n",
            "INFO:gguf.vocab:Adding 100000 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 100257\n",
            "INFO:gguf.vocab:Setting special token type eos to 100265\n",
            "INFO:gguf.vocab:Setting special token type unk to 5809\n",
            "INFO:gguf.vocab:Setting special token type pad to 100351\n",
            "INFO:gguf.vocab:Setting chat_template to {% for message in messages %}{% if (message['role'] == 'system') %}{{'<|im_start|>system<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'user') %}{{'<|im_start|>user<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'assistant') %}{{'<|im_start|>assistant<|im_sep|>' + message['content'] + '<|im_end|>'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant<|im_sep|>' }}{% endif %}\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/home/youcef/Documents/unsloth/model/unsloth.BF16.gguf: n_tensors = 363, total_size = 29.3G\n",
            "Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29.3G/29.3G [04:37<00:00, 106Mbyte/s] \n",
            "INFO:hf-to-gguf:Model successfully exported to /home/youcef/Documents/unsloth/model/unsloth.BF16.gguf\n",
            "Unsloth: Conversion completed! Output location: /home/youcef/Documents/unsloth/model/unsloth.BF16.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q4_k_m. This might take 20 minutes...\n",
            "main: build = 5028 (e0e912f4)\n",
            "main: built with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n",
            "main: quantizing '/home/youcef/Documents/unsloth/model/unsloth.BF16.gguf' to '/home/youcef/Documents/unsloth/model/unsloth.Q4_K_M.gguf' as Q4_K_M using 40 threads\n",
            "llama_model_loader: loaded meta data with 29 key-value pairs and 363 tensors from /home/youcef/Documents/unsloth/model/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Model\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 15B\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                       llama.context_length u32              = 16384\n",
            "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 17920\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 10\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 250000.000000\n",
            "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  14:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 100352\n",
            "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = dbrx\n",
            "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,100352]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,100352]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,100000]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\n",
            "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 100257\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 100265\n",
            "llama_model_loader: - kv  24:            tokenizer.ggml.unknown_token_id u32              = 5809\n",
            "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 100351\n",
            "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% for message in messages %}{% if (m...\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type bf16:  282 tensors\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
            "[   1/ 363]                        output.weight - [ 5120, 100352,     1,     1], type =   bf16, converting to q6_K .. size =   980.00 MiB ->   401.95 MiB\n",
            "[   2/ 363]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   3/ 363]                    token_embd.weight - [ 5120, 100352,     1,     1], type =   bf16, converting to q4_K .. size =   980.00 MiB ->   275.62 MiB\n",
            "[   4/ 363]                  blk.0.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[   5/ 363]               blk.0.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   6/ 363]             blk.0.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[   7/ 363]                  blk.0.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[   8/ 363]                  blk.0.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[   9/ 363]                blk.0.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  10/ 363]                blk.0.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  11/ 363]                blk.0.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  12/ 363]                  blk.0.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  13/ 363]                  blk.1.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  14/ 363]               blk.1.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  15/ 363]             blk.1.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  16/ 363]                  blk.1.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  17/ 363]                  blk.1.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  18/ 363]                blk.1.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  19/ 363]                blk.1.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  20/ 363]                blk.1.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  21/ 363]                  blk.1.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  22/ 363]                  blk.2.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  23/ 363]               blk.2.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  24/ 363]             blk.2.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  25/ 363]                  blk.2.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  26/ 363]                  blk.2.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  27/ 363]                blk.2.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  28/ 363]                blk.2.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  29/ 363]                blk.2.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  30/ 363]                  blk.2.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  31/ 363]                  blk.3.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  32/ 363]               blk.3.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  33/ 363]             blk.3.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  34/ 363]                  blk.3.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  35/ 363]                  blk.3.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  36/ 363]                blk.3.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  37/ 363]                blk.3.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  38/ 363]                blk.3.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  39/ 363]                  blk.3.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  40/ 363]                  blk.4.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  41/ 363]               blk.4.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  42/ 363]             blk.4.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  43/ 363]                  blk.4.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  44/ 363]                  blk.4.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  45/ 363]                blk.4.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  46/ 363]                blk.4.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  47/ 363]                blk.4.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  48/ 363]                  blk.4.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  49/ 363]                  blk.5.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  50/ 363]               blk.5.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  51/ 363]             blk.5.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  52/ 363]                  blk.5.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  53/ 363]                  blk.5.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  54/ 363]                blk.5.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  55/ 363]                blk.5.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  56/ 363]                blk.5.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  57/ 363]                  blk.5.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  58/ 363]                  blk.6.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  59/ 363]               blk.6.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  60/ 363]             blk.6.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  61/ 363]                  blk.6.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  62/ 363]                  blk.6.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  63/ 363]                blk.6.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  64/ 363]                blk.6.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  65/ 363]                blk.6.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  66/ 363]                  blk.6.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  67/ 363]                  blk.7.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  68/ 363]               blk.7.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  69/ 363]             blk.7.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  70/ 363]                  blk.7.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  71/ 363]                  blk.7.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  72/ 363]                blk.7.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  73/ 363]                blk.7.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  74/ 363]                blk.7.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  75/ 363]                  blk.7.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  76/ 363]                  blk.8.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  77/ 363]               blk.8.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  78/ 363]             blk.8.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  79/ 363]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  80/ 363]                  blk.8.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  81/ 363]                blk.8.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  82/ 363]                blk.8.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  83/ 363]                blk.8.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  84/ 363]                  blk.8.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  85/ 363]                  blk.9.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  86/ 363]               blk.9.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  87/ 363]             blk.9.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  88/ 363]                  blk.9.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  89/ 363]                  blk.9.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  90/ 363]                blk.9.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  91/ 363]                blk.9.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  92/ 363]                blk.9.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  93/ 363]                  blk.9.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[  94/ 363]                 blk.10.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[  95/ 363]              blk.10.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  96/ 363]            blk.10.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  97/ 363]                 blk.10.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[  98/ 363]                 blk.10.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  99/ 363]               blk.10.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 100/ 363]               blk.10.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 101/ 363]               blk.10.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 102/ 363]                 blk.10.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 103/ 363]                 blk.11.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 104/ 363]              blk.11.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 105/ 363]            blk.11.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 106/ 363]                 blk.11.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 107/ 363]                 blk.11.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 108/ 363]               blk.11.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 109/ 363]               blk.11.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 110/ 363]               blk.11.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 111/ 363]                 blk.11.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 112/ 363]                 blk.12.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 113/ 363]              blk.12.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 114/ 363]            blk.12.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 115/ 363]                 blk.12.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 116/ 363]                 blk.12.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 117/ 363]               blk.12.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 118/ 363]               blk.12.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 119/ 363]               blk.12.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 120/ 363]                 blk.12.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 121/ 363]                 blk.13.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 122/ 363]              blk.13.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 123/ 363]            blk.13.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 124/ 363]                 blk.13.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 125/ 363]                 blk.13.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 126/ 363]               blk.13.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 127/ 363]               blk.13.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 128/ 363]               blk.13.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 129/ 363]                 blk.13.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 130/ 363]                 blk.14.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 131/ 363]              blk.14.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 132/ 363]            blk.14.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 133/ 363]                 blk.14.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 134/ 363]                 blk.14.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 135/ 363]               blk.14.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 136/ 363]               blk.14.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 137/ 363]               blk.14.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 138/ 363]                 blk.14.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 139/ 363]                 blk.15.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 140/ 363]              blk.15.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 141/ 363]            blk.15.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 142/ 363]                 blk.15.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 143/ 363]                 blk.15.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 144/ 363]               blk.15.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 145/ 363]               blk.15.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 146/ 363]               blk.15.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 147/ 363]                 blk.15.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 148/ 363]                 blk.16.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 149/ 363]              blk.16.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 150/ 363]            blk.16.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 151/ 363]                 blk.16.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 152/ 363]                 blk.16.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 153/ 363]               blk.16.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 154/ 363]               blk.16.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 155/ 363]               blk.16.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 156/ 363]                 blk.16.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 157/ 363]                 blk.17.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 158/ 363]              blk.17.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 159/ 363]            blk.17.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 160/ 363]                 blk.17.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 161/ 363]                 blk.17.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 162/ 363]               blk.17.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 163/ 363]               blk.17.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 164/ 363]               blk.17.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 165/ 363]                 blk.17.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 166/ 363]                 blk.18.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 167/ 363]              blk.18.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 168/ 363]            blk.18.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 169/ 363]                 blk.18.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 170/ 363]                 blk.18.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 171/ 363]               blk.18.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 172/ 363]               blk.18.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 173/ 363]               blk.18.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 174/ 363]                 blk.18.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 175/ 363]                 blk.19.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 176/ 363]              blk.19.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 177/ 363]            blk.19.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 178/ 363]                 blk.19.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 179/ 363]                 blk.19.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 180/ 363]               blk.19.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 181/ 363]               blk.19.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 182/ 363]               blk.19.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 183/ 363]                 blk.19.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 184/ 363]                 blk.20.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 185/ 363]              blk.20.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 186/ 363]            blk.20.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 187/ 363]                 blk.20.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 188/ 363]                 blk.20.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 189/ 363]               blk.20.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 190/ 363]               blk.20.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 191/ 363]               blk.20.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 192/ 363]                 blk.20.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 193/ 363]                 blk.21.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 194/ 363]              blk.21.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 195/ 363]            blk.21.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 196/ 363]                 blk.21.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 197/ 363]                 blk.21.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 198/ 363]               blk.21.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 199/ 363]               blk.21.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 200/ 363]               blk.21.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 201/ 363]                 blk.21.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 202/ 363]                 blk.22.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 203/ 363]              blk.22.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 204/ 363]            blk.22.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 205/ 363]                 blk.22.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 206/ 363]                 blk.22.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 207/ 363]               blk.22.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 208/ 363]               blk.22.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 209/ 363]               blk.22.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 210/ 363]                 blk.22.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 211/ 363]                 blk.23.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 212/ 363]              blk.23.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 213/ 363]            blk.23.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 214/ 363]                 blk.23.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 215/ 363]                 blk.23.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 216/ 363]               blk.23.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 217/ 363]               blk.23.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 218/ 363]               blk.23.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 219/ 363]                 blk.23.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 220/ 363]                 blk.24.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 221/ 363]              blk.24.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 222/ 363]            blk.24.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 223/ 363]                 blk.24.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 224/ 363]                 blk.24.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 225/ 363]               blk.24.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 226/ 363]               blk.24.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 227/ 363]               blk.24.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 228/ 363]                 blk.24.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 229/ 363]                 blk.25.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 230/ 363]              blk.25.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 231/ 363]            blk.25.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 232/ 363]                 blk.25.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 233/ 363]                 blk.25.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 234/ 363]               blk.25.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 235/ 363]               blk.25.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 236/ 363]               blk.25.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 237/ 363]                 blk.25.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 238/ 363]                 blk.26.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 239/ 363]              blk.26.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 240/ 363]            blk.26.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 241/ 363]                 blk.26.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 242/ 363]                 blk.26.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 243/ 363]               blk.26.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 244/ 363]               blk.26.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 245/ 363]               blk.26.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 246/ 363]                 blk.26.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 247/ 363]                 blk.27.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 248/ 363]              blk.27.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 249/ 363]            blk.27.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 250/ 363]                 blk.27.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 251/ 363]                 blk.27.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 252/ 363]               blk.27.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 253/ 363]               blk.27.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 254/ 363]               blk.27.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 255/ 363]                 blk.27.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 256/ 363]                 blk.28.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 257/ 363]              blk.28.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 258/ 363]            blk.28.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 259/ 363]                 blk.28.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 260/ 363]                 blk.28.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 261/ 363]               blk.28.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 262/ 363]               blk.28.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 263/ 363]               blk.28.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 264/ 363]                 blk.28.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 265/ 363]                 blk.29.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 266/ 363]              blk.29.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 267/ 363]            blk.29.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 268/ 363]                 blk.29.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 269/ 363]                 blk.29.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 270/ 363]               blk.29.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 271/ 363]               blk.29.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 272/ 363]               blk.29.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 273/ 363]                 blk.29.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 274/ 363]                 blk.30.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 275/ 363]              blk.30.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 276/ 363]            blk.30.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 277/ 363]                 blk.30.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 278/ 363]                 blk.30.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 279/ 363]               blk.30.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 280/ 363]               blk.30.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 281/ 363]               blk.30.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 282/ 363]                 blk.30.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 283/ 363]                 blk.31.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 284/ 363]              blk.31.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 285/ 363]            blk.31.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 286/ 363]                 blk.31.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 287/ 363]                 blk.31.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 288/ 363]               blk.31.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 289/ 363]               blk.31.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 290/ 363]               blk.31.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 291/ 363]                 blk.31.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 292/ 363]                 blk.32.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 293/ 363]              blk.32.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 294/ 363]            blk.32.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 295/ 363]                 blk.32.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 296/ 363]                 blk.32.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 297/ 363]               blk.32.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 298/ 363]               blk.32.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 299/ 363]               blk.32.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 300/ 363]                 blk.32.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 301/ 363]                 blk.33.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 302/ 363]              blk.33.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 303/ 363]            blk.33.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 304/ 363]                 blk.33.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 305/ 363]                 blk.33.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 306/ 363]               blk.33.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 307/ 363]               blk.33.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 308/ 363]               blk.33.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 309/ 363]                 blk.33.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 310/ 363]                 blk.34.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 311/ 363]              blk.34.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 312/ 363]            blk.34.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 313/ 363]                 blk.34.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 314/ 363]                 blk.34.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 315/ 363]               blk.34.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 316/ 363]               blk.34.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 317/ 363]               blk.34.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 318/ 363]                 blk.34.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 319/ 363]                 blk.35.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 320/ 363]              blk.35.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 321/ 363]            blk.35.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 322/ 363]                 blk.35.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 323/ 363]                 blk.35.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 324/ 363]               blk.35.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 325/ 363]               blk.35.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 326/ 363]               blk.35.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 327/ 363]                 blk.35.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 328/ 363]                 blk.36.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 329/ 363]              blk.36.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 330/ 363]            blk.36.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 331/ 363]                 blk.36.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 332/ 363]                 blk.36.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 333/ 363]               blk.36.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 334/ 363]               blk.36.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 335/ 363]               blk.36.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 336/ 363]                 blk.36.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 337/ 363]                 blk.37.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 338/ 363]              blk.37.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 339/ 363]            blk.37.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 340/ 363]                 blk.37.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 341/ 363]                 blk.37.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 342/ 363]               blk.37.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 343/ 363]               blk.37.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 344/ 363]               blk.37.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 345/ 363]                 blk.37.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 346/ 363]                 blk.38.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 347/ 363]              blk.38.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 348/ 363]            blk.38.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 349/ 363]                 blk.38.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 350/ 363]                 blk.38.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 351/ 363]               blk.38.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 352/ 363]               blk.38.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 353/ 363]               blk.38.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 354/ 363]                 blk.38.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 355/ 363]                 blk.39.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q4_K .. size =    12.50 MiB ->     3.52 MiB\n",
            "[ 356/ 363]              blk.39.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 357/ 363]            blk.39.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 358/ 363]                 blk.39.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q4_K .. size =    50.00 MiB ->    14.06 MiB\n",
            "[ 359/ 363]                 blk.39.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 360/ 363]               blk.39.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 361/ 363]               blk.39.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "[ 362/ 363]               blk.39.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 363/ 363]                 blk.39.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q4_K .. size =   175.00 MiB ->    49.22 MiB\n",
            "llama_model_quantize_impl: model size  = 27961.58 MB\n",
            "llama_model_quantize_impl: quant size  =  8475.06 MB\n",
            "\n",
            "main: quantize time = 78248.20 ms\n",
            "main:    total time = 78248.20 ms\n",
            "Unsloth: Conversion completed! Output location: /home/youcef/Documents/unsloth/model/unsloth.Q4_K_M.gguf\n",
            "Unsloth: [2] Converting GGUF 16bit into q5_k_m. This might take 20 minutes...\n",
            "main: build = 5028 (e0e912f4)\n",
            "main: built with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n",
            "main: quantizing '/home/youcef/Documents/unsloth/model/unsloth.BF16.gguf' to '/home/youcef/Documents/unsloth/model/unsloth.Q5_K_M.gguf' as Q5_K_M using 40 threads\n",
            "llama_model_loader: loaded meta data with 29 key-value pairs and 363 tensors from /home/youcef/Documents/unsloth/model/unsloth.BF16.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Model\n",
            "llama_model_loader: - kv   3:                         general.size_label str              = 15B\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
            "llama_model_loader: - kv   5:                       llama.context_length u32              = 16384\n",
            "llama_model_loader: - kv   6:                     llama.embedding_length u32              = 5120\n",
            "llama_model_loader: - kv   7:                  llama.feed_forward_length u32              = 17920\n",
            "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 40\n",
            "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 10\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 250000.000000\n",
            "llama_model_loader: - kv  11:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  12:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  13:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  14:                          general.file_type u32              = 32\n",
            "llama_model_loader: - kv  15:                           llama.vocab_size u32              = 100352\n",
            "llama_model_loader: - kv  16:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  17:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  18:                         tokenizer.ggml.pre str              = dbrx\n",
            "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,100352]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  20:                  tokenizer.ggml.token_type arr[i32,100352]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  21:                      tokenizer.ggml.merges arr[str,100000]  = [\"ƒ† ƒ†\", \"ƒ†ƒ† ƒ†ƒ†\", \"i n\", \"ƒ† t\",...\n",
            "llama_model_loader: - kv  22:                tokenizer.ggml.bos_token_id u32              = 100257\n",
            "llama_model_loader: - kv  23:                tokenizer.ggml.eos_token_id u32              = 100265\n",
            "llama_model_loader: - kv  24:            tokenizer.ggml.unknown_token_id u32              = 5809\n",
            "llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 100351\n",
            "llama_model_loader: - kv  26:                    tokenizer.chat_template str              = {% for message in messages %}{% if (m...\n",
            "llama_model_loader: - kv  27:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   81 tensors\n",
            "llama_model_loader: - type bf16:  282 tensors\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes\n",
            "[   1/ 363]                        output.weight - [ 5120, 100352,     1,     1], type =   bf16, converting to q6_K .. size =   980.00 MiB ->   401.95 MiB\n",
            "[   2/ 363]                   output_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   3/ 363]                    token_embd.weight - [ 5120, 100352,     1,     1], type =   bf16, converting to q5_K .. size =   980.00 MiB ->   336.88 MiB\n",
            "[   4/ 363]                  blk.0.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[   5/ 363]               blk.0.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[   6/ 363]             blk.0.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[   7/ 363]                  blk.0.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[   8/ 363]                  blk.0.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[   9/ 363]                blk.0.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  10/ 363]                blk.0.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  11/ 363]                blk.0.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  12/ 363]                  blk.0.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  13/ 363]                  blk.1.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[  14/ 363]               blk.1.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  15/ 363]             blk.1.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  16/ 363]                  blk.1.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  17/ 363]                  blk.1.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  18/ 363]                blk.1.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  19/ 363]                blk.1.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  20/ 363]                blk.1.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  21/ 363]                  blk.1.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  22/ 363]                  blk.2.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[  23/ 363]               blk.2.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  24/ 363]             blk.2.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  25/ 363]                  blk.2.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  26/ 363]                  blk.2.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  27/ 363]                blk.2.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  28/ 363]                blk.2.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  29/ 363]                blk.2.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  30/ 363]                  blk.2.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  31/ 363]                  blk.3.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[  32/ 363]               blk.3.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  33/ 363]             blk.3.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  34/ 363]                  blk.3.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  35/ 363]                  blk.3.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  36/ 363]                blk.3.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  37/ 363]                blk.3.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  38/ 363]                blk.3.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  39/ 363]                  blk.3.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  40/ 363]                  blk.4.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[  41/ 363]               blk.4.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  42/ 363]             blk.4.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  43/ 363]                  blk.4.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  44/ 363]                  blk.4.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  45/ 363]                blk.4.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  46/ 363]                blk.4.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  47/ 363]                blk.4.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  48/ 363]                  blk.4.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  49/ 363]                  blk.5.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[  50/ 363]               blk.5.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  51/ 363]             blk.5.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  52/ 363]                  blk.5.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  53/ 363]                  blk.5.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[  54/ 363]                blk.5.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  55/ 363]                blk.5.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  56/ 363]                blk.5.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  57/ 363]                  blk.5.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  58/ 363]                  blk.6.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[  59/ 363]               blk.6.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  60/ 363]             blk.6.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  61/ 363]                  blk.6.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  62/ 363]                  blk.6.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[  63/ 363]                blk.6.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  64/ 363]                blk.6.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  65/ 363]                blk.6.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  66/ 363]                  blk.6.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  67/ 363]                  blk.7.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[  68/ 363]               blk.7.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  69/ 363]             blk.7.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  70/ 363]                  blk.7.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  71/ 363]                  blk.7.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  72/ 363]                blk.7.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[  73/ 363]                blk.7.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  74/ 363]                blk.7.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  75/ 363]                  blk.7.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  76/ 363]                  blk.8.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[  77/ 363]               blk.8.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  78/ 363]             blk.8.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  79/ 363]                  blk.8.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  80/ 363]                  blk.8.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[  81/ 363]                blk.8.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  82/ 363]                blk.8.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  83/ 363]                blk.8.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  84/ 363]                  blk.8.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  85/ 363]                  blk.9.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[  86/ 363]               blk.9.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  87/ 363]             blk.9.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  88/ 363]                  blk.9.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  89/ 363]                  blk.9.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[  90/ 363]                blk.9.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  91/ 363]                blk.9.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  92/ 363]                blk.9.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  93/ 363]                  blk.9.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[  94/ 363]                 blk.10.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[  95/ 363]              blk.10.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[  96/ 363]            blk.10.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  97/ 363]                 blk.10.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[  98/ 363]                 blk.10.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[  99/ 363]               blk.10.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 100/ 363]               blk.10.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 101/ 363]               blk.10.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 102/ 363]                 blk.10.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 103/ 363]                 blk.11.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 104/ 363]              blk.11.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 105/ 363]            blk.11.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 106/ 363]                 blk.11.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 107/ 363]                 blk.11.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 108/ 363]               blk.11.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 109/ 363]               blk.11.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 110/ 363]               blk.11.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 111/ 363]                 blk.11.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 112/ 363]                 blk.12.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 113/ 363]              blk.12.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 114/ 363]            blk.12.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 115/ 363]                 blk.12.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 116/ 363]                 blk.12.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 117/ 363]               blk.12.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 118/ 363]               blk.12.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 119/ 363]               blk.12.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 120/ 363]                 blk.12.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 121/ 363]                 blk.13.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 122/ 363]              blk.13.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 123/ 363]            blk.13.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 124/ 363]                 blk.13.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 125/ 363]                 blk.13.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 126/ 363]               blk.13.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 127/ 363]               blk.13.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 128/ 363]               blk.13.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 129/ 363]                 blk.13.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 130/ 363]                 blk.14.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 131/ 363]              blk.14.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 132/ 363]            blk.14.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 133/ 363]                 blk.14.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 134/ 363]                 blk.14.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 135/ 363]               blk.14.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 136/ 363]               blk.14.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 137/ 363]               blk.14.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 138/ 363]                 blk.14.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 139/ 363]                 blk.15.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 140/ 363]              blk.15.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 141/ 363]            blk.15.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 142/ 363]                 blk.15.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 143/ 363]                 blk.15.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 144/ 363]               blk.15.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 145/ 363]               blk.15.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 146/ 363]               blk.15.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 147/ 363]                 blk.15.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 148/ 363]                 blk.16.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 149/ 363]              blk.16.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 150/ 363]            blk.16.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 151/ 363]                 blk.16.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 152/ 363]                 blk.16.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 153/ 363]               blk.16.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 154/ 363]               blk.16.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 155/ 363]               blk.16.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 156/ 363]                 blk.16.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 157/ 363]                 blk.17.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 158/ 363]              blk.17.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 159/ 363]            blk.17.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 160/ 363]                 blk.17.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 161/ 363]                 blk.17.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 162/ 363]               blk.17.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 163/ 363]               blk.17.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 164/ 363]               blk.17.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 165/ 363]                 blk.17.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 166/ 363]                 blk.18.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 167/ 363]              blk.18.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 168/ 363]            blk.18.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 169/ 363]                 blk.18.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 170/ 363]                 blk.18.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 171/ 363]               blk.18.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 172/ 363]               blk.18.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 173/ 363]               blk.18.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 174/ 363]                 blk.18.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 175/ 363]                 blk.19.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 176/ 363]              blk.19.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 177/ 363]            blk.19.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 178/ 363]                 blk.19.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 179/ 363]                 blk.19.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 180/ 363]               blk.19.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 181/ 363]               blk.19.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 182/ 363]               blk.19.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 183/ 363]                 blk.19.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 184/ 363]                 blk.20.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 185/ 363]              blk.20.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 186/ 363]            blk.20.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 187/ 363]                 blk.20.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 188/ 363]                 blk.20.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 189/ 363]               blk.20.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 190/ 363]               blk.20.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 191/ 363]               blk.20.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 192/ 363]                 blk.20.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 193/ 363]                 blk.21.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 194/ 363]              blk.21.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 195/ 363]            blk.21.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 196/ 363]                 blk.21.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 197/ 363]                 blk.21.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 198/ 363]               blk.21.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 199/ 363]               blk.21.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 200/ 363]               blk.21.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 201/ 363]                 blk.21.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 202/ 363]                 blk.22.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 203/ 363]              blk.22.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 204/ 363]            blk.22.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 205/ 363]                 blk.22.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 206/ 363]                 blk.22.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 207/ 363]               blk.22.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 208/ 363]               blk.22.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 209/ 363]               blk.22.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 210/ 363]                 blk.22.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 211/ 363]                 blk.23.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 212/ 363]              blk.23.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 213/ 363]            blk.23.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 214/ 363]                 blk.23.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 215/ 363]                 blk.23.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 216/ 363]               blk.23.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 217/ 363]               blk.23.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 218/ 363]               blk.23.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 219/ 363]                 blk.23.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 220/ 363]                 blk.24.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 221/ 363]              blk.24.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 222/ 363]            blk.24.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 223/ 363]                 blk.24.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 224/ 363]                 blk.24.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 225/ 363]               blk.24.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 226/ 363]               blk.24.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 227/ 363]               blk.24.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 228/ 363]                 blk.24.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 229/ 363]                 blk.25.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 230/ 363]              blk.25.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 231/ 363]            blk.25.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 232/ 363]                 blk.25.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 233/ 363]                 blk.25.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 234/ 363]               blk.25.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 235/ 363]               blk.25.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 236/ 363]               blk.25.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 237/ 363]                 blk.25.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 238/ 363]                 blk.26.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 239/ 363]              blk.26.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 240/ 363]            blk.26.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 241/ 363]                 blk.26.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 242/ 363]                 blk.26.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 243/ 363]               blk.26.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 244/ 363]               blk.26.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 245/ 363]               blk.26.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 246/ 363]                 blk.26.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 247/ 363]                 blk.27.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 248/ 363]              blk.27.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 249/ 363]            blk.27.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 250/ 363]                 blk.27.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 251/ 363]                 blk.27.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 252/ 363]               blk.27.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 253/ 363]               blk.27.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 254/ 363]               blk.27.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 255/ 363]                 blk.27.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 256/ 363]                 blk.28.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 257/ 363]              blk.28.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 258/ 363]            blk.28.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 259/ 363]                 blk.28.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 260/ 363]                 blk.28.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 261/ 363]               blk.28.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 262/ 363]               blk.28.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 263/ 363]               blk.28.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 264/ 363]                 blk.28.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 265/ 363]                 blk.29.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 266/ 363]              blk.29.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 267/ 363]            blk.29.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 268/ 363]                 blk.29.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 269/ 363]                 blk.29.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 270/ 363]               blk.29.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 271/ 363]               blk.29.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 272/ 363]               blk.29.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 273/ 363]                 blk.29.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 274/ 363]                 blk.30.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 275/ 363]              blk.30.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 276/ 363]            blk.30.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 277/ 363]                 blk.30.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 278/ 363]                 blk.30.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 279/ 363]               blk.30.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 280/ 363]               blk.30.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 281/ 363]               blk.30.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 282/ 363]                 blk.30.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 283/ 363]                 blk.31.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 284/ 363]              blk.31.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 285/ 363]            blk.31.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 286/ 363]                 blk.31.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 287/ 363]                 blk.31.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 288/ 363]               blk.31.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 289/ 363]               blk.31.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 290/ 363]               blk.31.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 291/ 363]                 blk.31.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 292/ 363]                 blk.32.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 293/ 363]              blk.32.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 294/ 363]            blk.32.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 295/ 363]                 blk.32.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 296/ 363]                 blk.32.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 297/ 363]               blk.32.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 298/ 363]               blk.32.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 299/ 363]               blk.32.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 300/ 363]                 blk.32.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 301/ 363]                 blk.33.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 302/ 363]              blk.33.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 303/ 363]            blk.33.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 304/ 363]                 blk.33.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 305/ 363]                 blk.33.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 306/ 363]               blk.33.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 307/ 363]               blk.33.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 308/ 363]               blk.33.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 309/ 363]                 blk.33.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 310/ 363]                 blk.34.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 311/ 363]              blk.34.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 312/ 363]            blk.34.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 313/ 363]                 blk.34.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 314/ 363]                 blk.34.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 315/ 363]               blk.34.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 316/ 363]               blk.34.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 317/ 363]               blk.34.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 318/ 363]                 blk.34.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 319/ 363]                 blk.35.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 320/ 363]              blk.35.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 321/ 363]            blk.35.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 322/ 363]                 blk.35.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 323/ 363]                 blk.35.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 324/ 363]               blk.35.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 325/ 363]               blk.35.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 326/ 363]               blk.35.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 327/ 363]                 blk.35.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 328/ 363]                 blk.36.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 329/ 363]              blk.36.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 330/ 363]            blk.36.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 331/ 363]                 blk.36.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 332/ 363]                 blk.36.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 333/ 363]               blk.36.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 334/ 363]               blk.36.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 335/ 363]               blk.36.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 336/ 363]                 blk.36.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 337/ 363]                 blk.37.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 338/ 363]              blk.37.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 339/ 363]            blk.37.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 340/ 363]                 blk.37.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 341/ 363]                 blk.37.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 342/ 363]               blk.37.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 343/ 363]               blk.37.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 344/ 363]               blk.37.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 345/ 363]                 blk.37.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 346/ 363]                 blk.38.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 347/ 363]              blk.38.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 348/ 363]            blk.38.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 349/ 363]                 blk.38.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 350/ 363]                 blk.38.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 351/ 363]               blk.38.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 352/ 363]               blk.38.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 353/ 363]               blk.38.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 354/ 363]                 blk.38.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 355/ 363]                 blk.39.attn_k.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q5_K .. size =    12.50 MiB ->     4.30 MiB\n",
            "[ 356/ 363]              blk.39.attn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 357/ 363]            blk.39.attn_output.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 358/ 363]                 blk.39.attn_q.weight - [ 5120,  5120,     1,     1], type =   bf16, converting to q5_K .. size =    50.00 MiB ->    17.19 MiB\n",
            "[ 359/ 363]                 blk.39.attn_v.weight - [ 5120,  1280,     1,     1], type =   bf16, converting to q6_K .. size =    12.50 MiB ->     5.13 MiB\n",
            "[ 360/ 363]               blk.39.ffn_down.weight - [17920,  5120,     1,     1], type =   bf16, converting to q6_K .. size =   175.00 MiB ->    71.78 MiB\n",
            "[ 361/ 363]               blk.39.ffn_gate.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "[ 362/ 363]               blk.39.ffn_norm.weight - [ 5120,     1,     1,     1], type =    f32, size =    0.020 MB\n",
            "[ 363/ 363]                 blk.39.ffn_up.weight - [ 5120, 17920,     1,     1], type =   bf16, converting to q5_K .. size =   175.00 MiB ->    60.16 MiB\n",
            "llama_model_quantize_impl: model size  = 27961.58 MB\n",
            "llama_model_quantize_impl: quant size  =  9926.93 MB\n",
            "\n",
            "main: quantize time = 70587.30 ms\n",
            "main:    total time = 70587.30 ms\n",
            "Unsloth: Conversion completed! Output location: /home/youcef/Documents/unsloth/model/unsloth.Q5_K_M.gguf\n",
            "Unsloth: Saved Ollama Modelfile to model/Modelfile\n"
          ]
        }
      ],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if True: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = [\"q4_k_m\", \"q5_k_m\"])\n",
        "if False: model.push_to_hub_gguf(\"Youcef/Nodify-14B-v0.1-q4_k_m\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhsEDhOguSmC"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0262bf690a744e34b348a1a7efb74b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa980b3860db4e508140ec6073462cc9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7153553bfd334094bcb67f8f4cc5d991",
            "value": "Map:‚Äá100%"
          }
        },
        "02d641481a7649f08a3a4e8f370fcfea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05b0472385684402aca910ed2f0aabd9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b985f5ef06542d387f98e0b098e0924": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ef50772c7a94db69138dab0dd358f84": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1142781a8c3d4a1a91fc39a6f091984b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "177e19c2009a4c4cbe44b3bd2db483b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b5c4c7dfee84efeb977db2f599a8eaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60d69afaf00c4b2283507ff27e76327c",
            "max": 100000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57f56e28c0bd40c69e123d8e257197f0",
            "value": 100000
          }
        },
        "2352c2b232a642739042ef80ed58f340": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6395fbb46e514e5fb4c9799ed49d5e1d",
              "IPY_MODEL_d5b3294c4f364678995e3506c6d32851",
              "IPY_MODEL_6b90a4ded41040e8b5ecf1e7db3c4d0e"
            ],
            "layout": "IPY_MODEL_f713ffc3af7c43478b3ac3ab78902a2b"
          }
        },
        "27b1a069a17146ae9cb58a3df3bc5809": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29470a8b11b941ef83428b3aeb1a8b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cb3fc2097d94211a5fb330391e53cd1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f6989a923464d6485656c80a7c37176": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84dd0711c1f449f183402a05fef703d6",
              "IPY_MODEL_b4d63f8417e847f3b8a6f41a017bdf89",
              "IPY_MODEL_c64d6a3a71264a5c99f39ab67196d2af"
            ],
            "layout": "IPY_MODEL_68527325ed8c4d49a58b5466a84367fd"
          }
        },
        "3036b49d79774ff6ae809cf56a0af70d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30992645b93d4c54baf8cdad6c4c929a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32a99360bcc24ac084d5bb8717b546af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33c7d2c5e3994e86ba6acd49a8c06e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53458da298294ca0ad57b20a3f7c2e37",
            "max": 100000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_436f412c800749338a63f3b4af1ae1be",
            "value": 100000
          }
        },
        "3cae835ecd5442c8a06f06a0d3e53d6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ed2452a3ca440fd9f8daee03a7fd219": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd89881e45884f2a9cf09cc09ac6a199",
            "max": 100000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8410932ee414d12a0a687012421ae0b",
            "value": 100000
          }
        },
        "436f412c800749338a63f3b4af1ae1be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f71cb58efbd4f33b8a65f543fb4976c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f88fc189a0e4c3e8a77daeeb11ef5c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53458da298294ca0ad57b20a3f7c2e37": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53b54e72b7e547e38eddf9e73161e320": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57291316060f4f08b0b132272b726a02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_906c329a38804ce4bf550ae4d8600fa5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_81ff5b4df3844f3f8a148cd6b9c13d8a",
            "value": "‚Äá160k/160k‚Äá[00:00&lt;00:00,‚Äá1.92MB/s]"
          }
        },
        "57f56e28c0bd40c69e123d8e257197f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "60d69afaf00c4b2283507ff27e76327c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6102523f0979436f884c6e71159b66e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "623f6d9c54a6482c853233eb80799fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0262bf690a744e34b348a1a7efb74b1f",
              "IPY_MODEL_d13a77154efc4c3a85ae4411fa6ddf1c",
              "IPY_MODEL_8c2ca52c23b84a8486155de435810fbf"
            ],
            "layout": "IPY_MODEL_9e6ec29b77f448b392b8ff1a91997a8a"
          }
        },
        "635f4a58a057482c968b2a9701e626a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6395fbb46e514e5fb4c9799ed49d5e1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85b0e4fceb974083be38d2432758bbe4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c8f216c08a154083b8c3e57c5ae46789",
            "value": "model-00001-of-00003.safetensors:‚Äá‚Äá44%"
          }
        },
        "66624d56032a4dc4bfcd5114a0e25356": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81a33502ef3e406f82c972c37b2cdf55",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e423bfaabc124dbabf79a67a61bc2a35",
            "value": "Standardizing‚Äáformat:‚Äá100%"
          }
        },
        "68527325ed8c4d49a58b5466a84367fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a09a09ade794ced856efe7d44726925": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a3f08207d704c1e9186df24b359de83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b90a4ded41040e8b5ecf1e7db3c4d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_732a8e99a2b54250a897adf77dad92de",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_29470a8b11b941ef83428b3aeb1a8b56",
            "value": "‚Äá2.19G/4.97G‚Äá[00:15&lt;00:04,‚Äá566MB/s]"
          }
        },
        "6e8a7e86b3dd44088752e5a214397e11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30992645b93d4c54baf8cdad6c4c929a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4f88fc189a0e4c3e8a77daeeb11ef5c3",
            "value": "model.safetensors.index.json:‚Äá100%"
          }
        },
        "6e98be70d573433ca155e571e6a46c1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7153553bfd334094bcb67f8f4cc5d991": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72d316ffc8104f469f08667b47cb60b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "732a8e99a2b54250a897adf77dad92de": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73dcd49b09a3489ab562443560bc5774": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8abd482bccf1494595e509ee89dc80b9",
              "IPY_MODEL_1b5c4c7dfee84efeb977db2f599a8eaf",
              "IPY_MODEL_d5860fd902644163942457dcc885da75"
            ],
            "layout": "IPY_MODEL_7b9ab760372345c88630c7f4b9ff7094"
          }
        },
        "7529766ba1c14ec6bf6df35dcfc63218": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0adc437d17b4e118d7d89b35f732081",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6a09a09ade794ced856efe7d44726925",
            "value": "‚Äá100000/100000‚Äá[00:09&lt;00:00,‚Äá13050.41‚Äáexamples/s]"
          }
        },
        "764638d0a3e64b4292ad06bab89df2e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cb3fc2097d94211a5fb330391e53cd1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bbf5655c3e014fa69a71958d48747943",
            "value": "README.md:‚Äá100%"
          }
        },
        "780b64fb9953476585de3a34c734808b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78451176ff984817a90e2680ffda2871": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b9ab760372345c88630c7f4b9ff7094": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bfeafbc2fc843019187612ac61fda18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05b0472385684402aca910ed2f0aabd9",
            "max": 982,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_177e19c2009a4c4cbe44b3bd2db483b0",
            "value": 982
          }
        },
        "8047107bdb6840a08062d9fa5bfd9d0c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81a33502ef3e406f82c972c37b2cdf55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81fee9418ac8471c97267aa1654422e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81ff5b4df3844f3f8a148cd6b9c13d8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "834b807ac41c4039b1bb8c9701b348ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe48179cd7a54074b214ed8e3bda3108",
              "IPY_MODEL_33c7d2c5e3994e86ba6acd49a8c06e00",
              "IPY_MODEL_d540c030cbbc42f0a29a5c5a0cb6c04e"
            ],
            "layout": "IPY_MODEL_f729a6f7a3fb4b028754a8d3605673bd"
          }
        },
        "843025172bf4478a8e5e8586385d55a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84a035ac792d422592f253af976a8b6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84dd0711c1f449f183402a05fef703d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b985f5ef06542d387f98e0b098e0924",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6a3f08207d704c1e9186df24b359de83",
            "value": "train-00000-of-00001.parquet:‚Äá100%"
          }
        },
        "8594b57f93d74a98ab3022077cf2fec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66624d56032a4dc4bfcd5114a0e25356",
              "IPY_MODEL_3ed2452a3ca440fd9f8daee03a7fd219",
              "IPY_MODEL_7529766ba1c14ec6bf6df35dcfc63218"
            ],
            "layout": "IPY_MODEL_0ef50772c7a94db69138dab0dd358f84"
          }
        },
        "85b0e4fceb974083be38d2432758bbe4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8abd482bccf1494595e509ee89dc80b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cae835ecd5442c8a06f06a0d3e53d6c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d7fdf432609f4615beab4f71b3da92ec",
            "value": "Map:‚Äá100%"
          }
        },
        "8c2ca52c23b84a8486155de435810fbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e98be70d573433ca155e571e6a46c1e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_cfcd0cd808b4460185579afb6de0ece1",
            "value": "‚Äá100000/100000‚Äá[00:53&lt;00:00,‚Äá2283.60‚Äáexamples/s]"
          }
        },
        "906c329a38804ce4bf550ae4d8600fa5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90cdca8d013f41688dcc03967b47c377": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e6ec29b77f448b392b8ff1a91997a8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fcad95c256646cc8e40116fb52e73c0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1517d1f49ba40e0befaeccfb8c087fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4954bacd6d34aa283245fd19c2644fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e8a7e86b3dd44088752e5a214397e11",
              "IPY_MODEL_c6587efa49644386b5ae3c04766d9bc2",
              "IPY_MODEL_57291316060f4f08b0b132272b726a02"
            ],
            "layout": "IPY_MODEL_efebd10c2d5a41308bbff5569bcb4c25"
          }
        },
        "ad23d13c83ed4c728e3cc1ad9ea60415": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae77f6e84e69422bb914c0be452d86fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f9b25420835f430983b4bc81bdda34bb",
              "IPY_MODEL_b048e885cb0d45db957f94586152665c",
              "IPY_MODEL_c9804dfac114404dbc23e4f56f2979db"
            ],
            "layout": "IPY_MODEL_1142781a8c3d4a1a91fc39a6f091984b"
          }
        },
        "b048e885cb0d45db957f94586152665c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f59b04e983ed4bf1b6067a89a5364b87",
            "max": 100000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78451176ff984817a90e2680ffda2871",
            "value": 100000
          }
        },
        "b0a140846ba14742ac35bc556e8cc7d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4d63f8417e847f3b8a6f41a017bdf89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02d641481a7649f08a3a4e8f370fcfea",
            "max": 116531415,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53b54e72b7e547e38eddf9e73161e320",
            "value": 116531404
          }
        },
        "b8410932ee414d12a0a687012421ae0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ba616d7ac9e045df89781674d862cfb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_764638d0a3e64b4292ad06bab89df2e4",
              "IPY_MODEL_7bfeafbc2fc843019187612ac61fda18",
              "IPY_MODEL_f93bf0726a5a43aa8a2880fd8d488cb0"
            ],
            "layout": "IPY_MODEL_d62c498931cf4239af39ebb460ba871a"
          }
        },
        "bbf5655c3e014fa69a71958d48747943": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c64d6a3a71264a5c99f39ab67196d2af": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_780b64fb9953476585de3a34c734808b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4f71cb58efbd4f33b8a65f543fb4976c",
            "value": "‚Äá117M/117M‚Äá[00:01&lt;00:00,‚Äá87.1MB/s]"
          }
        },
        "c6587efa49644386b5ae3c04766d9bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fcad95c256646cc8e40116fb52e73c0",
            "max": 160171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f95441ef1e5c4900af12687a4b9770a3",
            "value": 160171
          }
        },
        "c8f216c08a154083b8c3e57c5ae46789": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9804dfac114404dbc23e4f56f2979db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72d316ffc8104f469f08667b47cb60b5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fd142660e4554cb58b4ffcc716e97cde",
            "value": "‚Äá100000/100000‚Äá[00:02&lt;00:00,‚Äá19135.12‚Äáexamples/s]"
          }
        },
        "cd89881e45884f2a9cf09cc09ac6a199": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfcd0cd808b4460185579afb6de0ece1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d13a77154efc4c3a85ae4411fa6ddf1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84a035ac792d422592f253af976a8b6b",
            "max": 100000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90cdca8d013f41688dcc03967b47c377",
            "value": 100000
          }
        },
        "d540c030cbbc42f0a29a5c5a0cb6c04e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da2af48e73ad499db66105c95690b378",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a1517d1f49ba40e0befaeccfb8c087fc",
            "value": "‚Äá100000/100000‚Äá[03:19&lt;00:00,‚Äá586.08‚Äáexamples/s]"
          }
        },
        "d5860fd902644163942457dcc885da75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8047107bdb6840a08062d9fa5bfd9d0c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_27b1a069a17146ae9cb58a3df3bc5809",
            "value": "‚Äá100000/100000‚Äá[00:13&lt;00:00,‚Äá13005.73‚Äáexamples/s]"
          }
        },
        "d5b3294c4f364678995e3506c6d32851": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32a99360bcc24ac084d5bb8717b546af",
            "max": 4971805503,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0a140846ba14742ac35bc556e8cc7d4",
            "value": 2191523631
          }
        },
        "d62c498931cf4239af39ebb460ba871a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7fdf432609f4615beab4f71b3da92ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da2af48e73ad499db66105c95690b378": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e423bfaabc124dbabf79a67a61bc2a35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "efebd10c2d5a41308bbff5569bcb4c25": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0adc437d17b4e118d7d89b35f732081": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f59b04e983ed4bf1b6067a89a5364b87": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f713ffc3af7c43478b3ac3ab78902a2b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f729a6f7a3fb4b028754a8d3605673bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f93bf0726a5a43aa8a2880fd8d488cb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_843025172bf4478a8e5e8586385d55a6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_81fee9418ac8471c97267aa1654422e6",
            "value": "‚Äá982/982‚Äá[00:00&lt;00:00,‚Äá20.3kB/s]"
          }
        },
        "f95441ef1e5c4900af12687a4b9770a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9b25420835f430983b4bc81bdda34bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6102523f0979436f884c6e71159b66e9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_635f4a58a057482c968b2a9701e626a6",
            "value": "Generating‚Äátrain‚Äásplit:‚Äá100%"
          }
        },
        "fa980b3860db4e508140ec6073462cc9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd142660e4554cb58b4ffcc716e97cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fe48179cd7a54074b214ed8e3bda3108": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad23d13c83ed4c728e3cc1ad9ea60415",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3036b49d79774ff6ae809cf56a0af70d",
            "value": "Map‚Äá(num_proc=2):‚Äá100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
